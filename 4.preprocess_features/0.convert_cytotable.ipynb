{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert SQLite file output from CellProfiler into parquet file using Cytotable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenna/mambaforge/envs/python_nuclear_speckle_env/lib/python3.9/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "/home/jenna/mambaforge/envs/python_nuclear_speckle_env/lib/python3.9/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# cytotable will merge objects from SQLite file into single cells and save as parquet file\n",
    "from cytotable import convert, presets\n",
    "\n",
    "# Set the logging level to a higher level to avoid outputting unnecessary errors from config file in convert function\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 plates in this dataset. Below are the names:\n",
      "slide2\n",
      "slide1\n",
      "slide4\n",
      "slide3\n"
     ]
    }
   ],
   "source": [
    "# type of file output for CytoTable\n",
    "dest_datatype = \"parquet\"\n",
    "\n",
    "# set main output dir for all parquet files\n",
    "output_dir = pathlib.Path(\"./data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# directory where SQLite files are located\n",
    "sqlite_dir = pathlib.Path(\"../3.cp_analysis/analysis_output\").resolve(strict=True)\n",
    "\n",
    "# Set converted parquet dir\n",
    "parquet_dir = pathlib.Path(f\"{output_dir}/converted_profiles\")\n",
    "parquet_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Set plate names as an empty list to append to\n",
    "plate_names = []\n",
    "\n",
    "# directory with plate maps\n",
    "platemap_dir = pathlib.Path(f\"../0.download_data/metadata/platemaps\")\n",
    "\n",
    "# list for plate names based on metadata files to use to create dictionary\n",
    "plate_names = []\n",
    "# iterate through metadata dir and append plate names from metadata files\n",
    "for file_path in platemap_dir.iterdir():\n",
    "    filename = file_path.stem\n",
    "    first_index = filename.split(\"_\")[0]\n",
    "    plate_names.append(first_index)\n",
    "\n",
    "# print the plate names and how many plates there are (confirmation)\n",
    "print(f\"There are {len(plate_names)} plates in this dataset. Below are the names:\")\n",
    "for name in plate_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run cytotable convert to output nuclei and image features separately for all plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversion with cytotable for plate: slide3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenna/mambaforge/envs/python_nuclear_speckle_env/lib/python3.9/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "/home/jenna/mambaforge/envs/python_nuclear_speckle_env/lib/python3.9/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion finished for plate: slide3\n",
      "Starting conversion with cytotable for plate: slide2\n",
      "Conversion finished for plate: slide2\n",
      "Starting conversion with cytotable for plate: slide1\n",
      "Conversion finished for plate: slide1\n",
      "Starting conversion with cytotable for plate: slide4\n",
      "Conversion finished for plate: slide4\n"
     ]
    }
   ],
   "source": [
    "# Iterate over directory with SQLite outputs\n",
    "for plate_folder in sqlite_dir.iterdir():\n",
    "    # Using the plate names list, only process files within that list\n",
    "    if plate_folder.name in plate_names:\n",
    "        # Construct output path for converted parquet file\n",
    "        output_path = pathlib.Path(f\"{parquet_dir}/{plate_folder.stem}/{plate_folder.stem}_converted.parquet\")\n",
    "        \n",
    "        print(\"Starting conversion with cytotable for plate:\", plate_folder.stem)\n",
    "\n",
    "        # merge single cells and output as parquet file\n",
    "        convert(\n",
    "            source_path=str(plate_folder),\n",
    "            dest_path=str(output_path),\n",
    "            dest_datatype=dest_datatype,\n",
    "            metadata=[\"image\"],\n",
    "            compartments=[\"nuclei\"],\n",
    "            identifying_columns=[\"ImageNumber\"],\n",
    "            joins=\"\"\"\n",
    "            SELECT\n",
    "                *\n",
    "            FROM\n",
    "                read_parquet('per_image.parquet') as per_image\n",
    "            INNER JOIN read_parquet('per_nuclei.parquet') AS per_nuclei ON\n",
    "                per_nuclei.Metadata_ImageNumber = per_image.Metadata_ImageNumber\n",
    "            \"\"\",\n",
    "            chunk_size=10000,\n",
    "        )\n",
    "\n",
    "        print(\"Conversion finished for plate:\", plate_folder.stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unwanted image + metadata columns and split the bulk and single-cell data from the main parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to edit image and nuclei data frames for plate: slide3\n",
      "Shape of nuclei data frame (58106, 585)\n",
      "Shape of image data frame (382, 313)\n",
      "Starting to edit image and nuclei data frames for plate: slide2\n",
      "Shape of nuclei data frame (53507, 585)\n",
      "Shape of image data frame (287, 313)\n",
      "Starting to edit image and nuclei data frames for plate: slide1\n",
      "Shape of nuclei data frame (71938, 585)\n",
      "Shape of image data frame (324, 313)\n",
      "Starting to edit image and nuclei data frames for plate: slide4\n",
      "Shape of nuclei data frame (65351, 585)\n",
      "Shape of image data frame (407, 313)\n"
     ]
    }
   ],
   "source": [
    "# path to unwanted image cols text file\n",
    "unwanted_list_path = pathlib.Path(\"./unwanted_image_cols.txt\")\n",
    "# Load the list of columns to remove from the text file\n",
    "with open(unwanted_list_path, \"r\") as file:\n",
    "    columns_to_remove = [line.strip() for line in file]\n",
    "\n",
    "# Iterate through directory with converted outputs\n",
    "for plate_folder in parquet_dir.iterdir():\n",
    "    # Only process the files that are in the plate names list\n",
    "    if plate_folder.name in plate_names:\n",
    "        # Read in each file as data frame\n",
    "        plate_df = pd.read_parquet(\n",
    "            pathlib.Path(f\"{plate_folder}/{plate_folder.stem}_converted.parquet\")\n",
    "        )\n",
    "        print(\n",
    "            \"Starting to edit image and nuclei data frames for plate:\",\n",
    "            plate_folder.stem,\n",
    "        )\n",
    "\n",
    "        # Drop the specified columns (ignore error if a column isn't there)\n",
    "        plate_df = plate_df.drop(columns=columns_to_remove, errors=\"ignore\")\n",
    "\n",
    "        # Identify metadata columns for nuclei data frame\n",
    "        metadata_columns = [\n",
    "            \"Metadata_ImageNumber\",\n",
    "            \"Image_Metadata_Plate\",\n",
    "            \"Image_Metadata_Site\",\n",
    "            \"Image_Metadata_Well\",\n",
    "            \"Image_Count_Nuclei\",\n",
    "            \"Image_FileName_DAPI\"\n",
    "        ]\n",
    "\n",
    "        # Create nuclei (single-cell) data frame\n",
    "        nuclei_df = plate_df[\n",
    "            metadata_columns\n",
    "            + [col for col in plate_df.columns if col.startswith(\"Nuclei_\")]\n",
    "        ]\n",
    "\n",
    "        # Create image (bulk) data frame\n",
    "        image_df = plate_df[\n",
    "            [\"Metadata_ImageNumber\"]\n",
    "            + [col for col in plate_df.columns if col.startswith(\"Image_\")]\n",
    "        ]\n",
    "        # Drop duplicate images in the image data frame since each image will have the same values even if the row is repeated\n",
    "        image_df = image_df.drop_duplicates(subset=\"Metadata_ImageNumber\")\n",
    "\n",
    "        # Save nuclei and image data frames to the same folder as the plate\n",
    "        nuclei_df.to_parquet(f\"{plate_folder}/per_nuclei.parquet\", index=False)\n",
    "        image_df.to_parquet(f\"{plate_folder}/per_image.parquet\", index=False)\n",
    "\n",
    "        # nuclei_df and image_df shape and one data frame to assess all looks correct\n",
    "        print(\"Shape of nuclei data frame\", nuclei_df.shape)\n",
    "        print(\"Shape of image data frame\", image_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
